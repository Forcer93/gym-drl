{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %pygame inline\n",
    "\n",
    "import os\n",
    "import gymnasium as gym\n",
    "# import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment example from OpenAI's Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'CartPole-v1'\n",
    "mode = 'rgb_array'\n",
    "env = gym.make(environment)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# observation, info = env.reset(seed=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Environment\n",
    "Important, five key components in RL: Agent, Action, Environment, Obervation, Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1, episodes+1):\n",
    "    \n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, terminated, truncated, info = env.step(action)\n",
    "#         score += reward\n",
    "\n",
    "#         if terminated or truncated:\n",
    "#             observation, info = env.reset()\n",
    "#             done = True\n",
    "\n",
    "#     print('Episode:{} Score:{}'.format(episode, score))\n",
    "# env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gymnasium.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO(policy='MlpPolicy', env=env, tensorboard_log=log_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_68\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 342  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008779552 |\n",
      "|    clip_fraction        | 0.0934      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.000877   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.15        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 50.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 330          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080711935 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | 0.0854       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0151      |\n",
      "|    value_loss           | 31.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 335         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008507049 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 51          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008340737 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 63.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008270084 |\n",
      "|    clip_fraction        | 0.0683      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 69.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007202301 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 49.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006486774 |\n",
      "|    clip_fraction        | 0.087       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.47        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004657509 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00865    |\n",
      "|    value_loss           | 42.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 344          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033803615 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.574       |\n",
      "|    explained_variance   | -0.00144     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.5         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000957    |\n",
      "|    value_loss           | 40.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x222a3e72530>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_PATH = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')\n",
    "model.save(PPO_PATH)\n",
    "\n",
    "del model\n",
    "\n",
    "model = PPO.load(PPO_PATH, env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "d:\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:208: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[155.]\n",
      "Episode:2 Score:[491.]\n",
      "Episode:3 Score:[405.]\n",
      "Episode:4 Score:[500.]\n",
      "Episode:5 Score:[216.]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    \n",
    "    observ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action, _ = model.predict(observ)\n",
    "        observ, reward, done, information = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            done = True\n",
    "\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a callback to the Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os\n",
    "\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "\n",
    "env = gym.make(environment)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=570.0, verbose=1)\n",
    "\n",
    "# stop_callback = print(\"stop_callback called\")\n",
    "# turn_callback = print('turn_callback called')\n",
    "eval_callback = EvalCallback(eval_env=env,\n",
    "                             callback_on_new_best=callback_on_best,\n",
    "                             eval_freq=5_000,\n",
    "                             best_model_save_path=save_path,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_77\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 444  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 322         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008398766 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.00302     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.34        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 54.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=466.80 +/- 66.40\n",
      "Episode length: 466.80 +/- 66.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 467         |\n",
      "|    mean_reward          | 467         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008885154 |\n",
      "|    clip_fraction        | 0.0623      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0893      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 287  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 304          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078097153 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.642       |\n",
      "|    explained_variance   | 0.262        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0162      |\n",
      "|    value_loss           | 56.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=319.00 +/- 94.82\n",
      "Episode length: 319.00 +/- 94.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 319         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006998604 |\n",
      "|    clip_fraction        | 0.0454      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 74.5        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 285   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 290          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051390845 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.596       |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.1         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00829     |\n",
      "|    value_loss           | 84.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 300          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050147497 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00762     |\n",
      "|    value_loss           | 78.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=395.00 +/- 99.00\n",
      "Episode length: 395.00 +/- 99.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 395         |\n",
      "|    mean_reward          | 395         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005364148 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00428    |\n",
      "|    value_loss           | 55.3        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 292   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 297          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044851433 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.91         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00385     |\n",
      "|    value_loss           | 28           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=425.40 +/- 76.92\n",
      "Episode length: 425.40 +/- 76.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 425         |\n",
      "|    mean_reward          | 425         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002854011 |\n",
      "|    clip_fraction        | 0.0119      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.02        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    value_loss           | 15.8        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 294   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 69    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x22398882410>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20_000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vieweing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO_2'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_2')\n",
    "training_log_path\n",
    "\n",
    "# !tensorboard --logdir={training_log_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
